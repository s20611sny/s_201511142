{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=\"D:\\\\Code\\\\201511142\\\\spark-2.0.0-bin-hadoop2.6\"\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).config('spark.sql.warehouse.dir','file:///D:/Code/201511142/data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "myList=list()\n",
    "print myList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]\n",
    "myDf=spark.createDataFrame(myList)\n",
    "myDf.printSchema()\n",
    "print myDf.take(1) #첫번째 줄 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=u'1', name=u'kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print spark.createDataFrame(myList, ['year','name','height']).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  espresso|\n",
      "| lee|     latte|\n",
      "| lim| americano|\n",
      "| kim|  affocato|\n",
      "| lee|long black|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%5]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])#random 으로 데이터 만듦\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)\n",
    "print \"row1: \",row1.year, row1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]\n",
    "\n",
    "myDf=spark.createDataFrame(myRows)\n",
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year=u'1', name=u'kim, js', height=170)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()\n",
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]\n",
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF()\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n",
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.where(rddDf._3 < 175)\\\n",
    "    .select([rddDf._1, rddDf._2]).show()\n",
    "rddDf.groupby(rddDf._1).max().show()  #필터역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim, js', year=1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))\n",
    "_myDf=spark.createDataFrame(_myRdd)\n",
    "_myDf.printSchema()\n",
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])\n",
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10| js1|\n",
      "| 20| js2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd,schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)\n",
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting D:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\conf\\spark-defaults.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile D:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\conf\\spark-defaults.conf\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "# Default system properties included when running spark-submit.\n",
    "# This is useful for setting default environmental settings.\n",
    "\n",
    "# Example:\n",
    "# spark.master                     spark://master:7077\n",
    "# spark.eventLog.enabled           true\n",
    "# spark.eventLog.dir               hdfs://namenode:8021/directory\n",
    "# spark.serializer                 org.apache.spark.serializer.KryoSerializer\n",
    "# spark.driver.memory              5g\n",
    "# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\n",
    "spark.jars.package= org.mongodb.spark:mongo-spark-connector_2.10:2.0.0, com.databricks:spark-csv_2.11:1.5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985 4.285136'.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext.textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "tDf=spark.createDataFrame(tRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tDf=tDf.withColumn(\"id\",tDf['_1'].cast(\"integer\")).drop('_1')\n",
    "tDf=tDf.withColumn(\"height\",tDf['_2'].cast(\"double\")).drop('_2')\n",
    "tDf=tDf.withColumn(\"weight\",tDf['_3'].cast(\"double\")).drop('_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 65.78  71.52  69.4   68.22  67.79]\n",
      "[ 112.99  136.49  153.03  142.34  144.3 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()\n",
    "print np.array(_weightRdd)[:5]\n",
    "print np.array(_heightRdd)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFhNJREFUeJzt3X2MXFd9xvHnqUmsBalsUi8hXju1QcZVQpAN07SV1TYQ\nCaeUYteoyKhSoURKoS4tiDq1iUSQqijbmiqqWgXJBTepFDk1EEzUFEKKpaZChGiM82aDi9skZCck\nXmoMajEmdn79Y+/G4/Xs3nm9r9+PZO3umbuTkxPnmXvP+d1zHRECAFTXz+XdAQDAaBH0AFBxBD0A\nVBxBDwAVR9ADQMUR9ABQcQQ9AFQcQQ8AFUfQA0DFvSLvDkjSsmXLYtWqVXl3AwBK5eDBgz+IiIm0\n4woR9KtWrVKz2cy7GwBQKraf6eY4pm4AoOIIegCoOIIeACqOoAeAiiPoAaDiClF1A6B49h9qadcD\nR/XcyVNaPj6m7RvXavP6yby7hT4Q9AAusP9QSzvvfUKnXjwrSWqdPKWd9z4hSYR9CTF1A+ACux44\n+nLIzzn14lnteuBoTj3CIAh6ABd47uSpntpRbAQ9gAssHx/rqR3FRtADuMD2jWs1dtGS89rGLlqi\n7RvX5tQjDILFWAAXmFtwpeqmGgh6AB1tXj9JsFcEUzcAUHEEPQBUXGrQ295j+7jtJ9vaPmm7ZfvR\n5M872l7bafuY7aO2N46q4wCA7nRzRn+npOs7tN8eEeuSP/8qSbavlLRV0lXJ79xhe0mH3wUAZCQ1\n6CPiIUknuny/TZLuiYjTEfGUpGOSrhmgfwCAAQ0yR/9h248nUzuXJG2Tkp5tO2Y6aQMA5KTfoP+0\npNdJWifp+5L+ptc3sH2j7abt5szMTJ/dAACk6SvoI+KFiDgbES9J+gedm55pSVrZduiKpK3Te+yO\niEZENCYmUh9iDgDoU19Bb/vyth9/V9JcRc59krbaXmp7taQ1kh4ZrIsAgEGk3hlre6+kayUtsz0t\n6RZJ19peJykkPS3pjyQpIg7b3ifpiKQzkrZFxNlO7wsAyIYjIu8+qNFoRLPZzLsbAFAqtg9GRCPt\nOO6MBYCKI+gBoOIIegCoOIIeACqOoAeAiiPoAaDiCHoAqDiCHgAqjmfGojT2H2rxsGqgDwQ9SmH/\noZZ23vuETr04u6NG6+Qp7bz3CUki7IEUTN2gFHY9cPTlkJ9z6sWz2vXA0Zx6BJQHQY9SeO7kqZ7a\nAZxD0KMUlo+P9dQO4ByCHqWwfeNajV10/nPmxy5aou0b12baj/2HWtowdUCrd9yvDVMHtP9Qx+fq\nAIXCYixKYW7BNc+qGxaEUVYEPUpj8/rJXAN1sQXhogR9UUpQi9IPzCLogS4VfUG4KFccRekHzmGO\nHuhSERaEF1sjKEoJalH6gXMIeqBLeS8Iz50pt06eUujcmfJc2BfliqMo/cA5BD3Qpc3rJ3Xblqs1\nOT4mS5ocH9NtW67ObDoi7Uy5CFccRepHJ3WtmmKOHujBoAvCgyxSpp0pb9+49ry5cSmfEtSi9GO+\nOq8dEPRAB6OoGhk0aJaPj6nVIeznzpSLUIJapH7MV4aqqVEh6FFoeZTpjerMb9Cg6eZMOe8S1KL1\no12d1w4IehRWXpfa3QRyPx9AgwZNUc+UOyliHX3aFVGVEfQorLwutdMCud8PoGEETRHPlOcr6lx4\nUdcOskDVDQorr0vttKqRfuvE8y7PzEpR6+jzrprKE2f0KKy8LrXTzvz6/QAq09TLIIo8F16GK6JR\nSA1623skvVPS8Yh447zXPibpU5ImIuIHSdtOSTdIOivpTyPigaH3GrWQ16V2WiAP8gFUh6Cp81x4\nUXVzRn+npL+X9E/tjbZXSnq7pO+1tV0paaukqyQtl/Rvtt8QEedfxwFdyPMMeLFArvNcbzfKPj5Z\nLSRnuWCdGvQR8ZDtVR1eul3STZK+1Na2SdI9EXFa0lO2j0m6RtI3Bu8q6qiIZ8B1mYLpVqfAum3L\n1aUcn6wWkrNesO5rjt72JkmtiHjMdvtLk5Iebvt5OmkDKqWIH0B5WCiwbttytb6+42059653WVV6\nZV1R1nPVje1XSvq4pE8M8g+2faPtpu3mzMzMIG8FICdFrbDpV1YLyVkvWPdTXvl6SaslPWb7aUkr\nJH3L9msltSStbDt2RdJ2gYjYHRGNiGhMTEz00Q0AeStyhU0/stqQLeuN33oO+oh4IiJeExGrImKV\nZqdn3hwRz0u6T9JW20ttr5a0RtIjQ+0xgMIo8k6V/cjqXoes76lIDXrbezW7mLrW9rTtGxY6NiIO\nS9on6Yikr0jaRsUNUF1Vuwksq5uqsr55yxExkjfuRaPRiGazmXc3APShiPvaDFtR/x1tH4yIRtpx\n3BkLYCBVr0Aq6t49vSDogSEp6lkfBlOFfewJemAIqnDWh86qUFnE7pXAEFStnhznVKGyiKAHhqAK\nZ33orAqVRUzdAEPAjo3dK9taRq97GxXx34+gB4ag7Ds2ZmWUaxmjDNhuK4uKulbD1A0wBHV+elEv\nRrWWMRewrZOnFDoXsPsPddyBZWSKulbDGT0wJFWvJx+GUa1lFKUEsqhrNZzRA8jMqCpYihKwRa3Q\nIeiBGtl/qKUNUwe0esf92jB1IPOpjVFVsBQlYItaoUPQAzVRhHnsUa1lFCVgi7pWw6ZmQE1smDrQ\nsQR0cnyslE+Dmq+IZY2jxqZmAM5TlHnsUWExfGFM3QA1UZR5bGSPoAdqoijz2MgeUzdATfR6Kz+q\ng6AHaoR57Hpi6gYAKo6gB4CKI+gBoOIIegCoOBZjUXt1vKMS9ULQo9b6eVAEHwwoG6ZuUGu9Piii\nCBuDAb0i6FFrve7/UtQnCAGLIehRa73u/1L1jcFQTalBb3uP7eO2n2xr+0vbj9t+1PZXbS9ve22n\n7WO2j9reOKqOA8PQ6/4vbAyGMurmjP5OSdfPa9sVEW+KiHWS/kXSJyTJ9pWStkq6KvmdO2wvEVBQ\nvT4ogo3BUEapVTcR8ZDtVfPaftz246skzT29ZJOkeyLitKSnbB+TdI2kbwylt0APuq2O6WX/FzYG\nQxn1XV5p+1ZJfyDpR5LemjRPSnq47bDppA2QlF1pYj9lk90a1cZglG1iVPpejI2ImyNipaS7Jf1J\nr79v+0bbTdvNmZmZfruBEsmyNLFs1TGUbWKUhlF1c7ekdyfftyStbHttRdJ2gYjYHRGNiGhMTEwM\noRsouizDt2zVMd2Ozf5DLW2YOqDVO+7XhqkDA38QDPv9UEx9Bb3tNW0/bpL0neT7+yRttb3U9mpJ\nayQ9MlgXURVZhm/ZqmO6GZthn/VzFVEf3ZRX7tXsYupa29O2b5A0ZftJ249LerukP5OkiDgsaZ+k\nI5K+ImlbRJxd4K1RM1mGb9mqY7oZm2FfEZVtegv9Sw36iHhvRFweERdFxIqI+GxEvDsi3piUWP5O\nRLTajr81Il4fEWsj4suj7T7KJMvw7bVsMm/djM2wr4jKNr2F/rGpGTKTdWlimR6b183YLB8fU6tD\nCPd7RTTs90NxEfTI1PxAm5smKEsgj1LaB9P2jWvPKxmVBrsiGvb7obgIemRqlPXtVTfsKyJu/qoP\nR0T6USPWaDSi2Wzm3Q1kYMPUgY7TBZPjY/r6jrfl0COgvGwfjIhG2nHsXolMsQAIZI+gR6bKVt8O\nVAFBj0yVrb4dqAIWY5GpKiwAsvkYyoagR+bKVN8+3zCrhvjAQFaYugF6MKxtA9hnBlnijL6EOBPM\nz7Cqhhb7wOC/JYaNM/qS4UwwX8OqGqLMFFki6EuGHQfzNayqoV4+MNgzHoMi6EuGM8HFjToUh7Ur\nZrcfGFzBYRiYoy8ZdhxcWFb76AyjaqjbMlPm8jEMBH3JsOPgwsoWit18YHAFh2Eg6EumCjccjUoV\nQ3GhK7jxV16kDVMH+DuArhD0JVTmG45GqYrTWp2u4C5aYv3vT8/ohz95URJbPSMdi7GojCruo9Np\n8fdVF79CL750/vbiVF5hMZzRozI2r59U85kT2vvNZ3U2Qktsvfst5b/6mX8Ft3rH/R2PK/MUFUaL\nM3pUxv5DLX3hYEtnk4fpnI3QFw62KleKyFbP6BVBj8qoy81kVZyiwmgxdYPKqGLVTSdUXqFXBD0q\nY9hVN0XePI7KK/SCqRtUxjCnNNh6AFXCGT3OU6Sz2F77MswpjbLdZQsshqDHy7LaK2aUfRnWlEZd\n5vtRD6lTN7b32D5u+8m2tl22v2P7cdtftD3e9tpO28dsH7W9cVQdx/AVqWol775Qwogq6WaO/k5J\n189re1DSGyPiTZL+U9JOSbJ9paStkq5KfucO20uEUijSWWzefaGEEVWSGvQR8ZCkE/PavhoRZ5If\nH5a0Ivl+k6R7IuJ0RDwl6Zika4bYX4xQkc5i8+7LsPadB4pgGHP0H5D0z8n3k5oN/jnTSRtKoEhb\nIBehL5QwoioGCnrbN0s6I+nuPn73Rkk3StIVV1wxSDcwJEW6EadIfQHKru+gt/1+Se+UdF1EzG2l\n15K0su2wFUnbBSJit6TdktRoNKLTMchekc5ii9QXoMz6umHK9vWSbpL0roj4SdtL90naanup7dWS\n1kh6ZPBuAgD6lXpGb3uvpGslLbM9LekWzVbZLJX0oG1JejgiPhgRh23vk3REs1M62yLibOd3BgBk\nwedmXfLTaDSi2Wzm3Q0AKBXbByOikXYce90AQMUR9ABQcQQ9AFQcm5oBKYq0oyfQD4IeWESRdvQE\n+sXUDbCIvHfRBIaBM3pgEaPaRZPpIGSJM3pgEaPYRZPHFCJrBD2wiFHsS890ELLG1A2wiFHsopn3\nQ1VQPwQ9kGLYu2guHx9Tq0Oo85hCjApTN0DGeEwhssYZPZAxHqqCrBH0QAejLn/koSrIEkFfUdRp\n94+7YVE1zNFXEHXag6H8EVVD0FcQQTUYyh9RNUzdVFC/QcV0zyzKH1E1nNFXUD+37TPdcw7lj6ga\ngr6C+gkqpnvO2bx+UrdtuVqT42OypMnxMd225eqOVzf7D7W0YeqAVu+4XxumDtTygxHFx9RNBfVT\np8289Pm6KX+kOgdlQdBXVK912sxL926xqyCCHkXC1A0kMS/dD66CUBYEPST1Ni+NWaPYqx4YBaZu\n8DJuy+/N9o1rz5ujl7gKQjER9ECf2JwMZUHQAwPgKghlkDpHb3uP7eO2n2xr+z3bh22/ZLsx7/id\nto/ZPmp74yg6DQDoXjeLsXdKun5e25OStkh6qL3R9pWStkq6KvmdO2wvEQAgN6lBHxEPSToxr+3b\nEdHplslNku6JiNMR8ZSkY5KuGUpPAQB9GXZ55aSkZ9t+nk7aAAA5ya2O3vaNtpu2mzMzM3l1AwAq\nb9hB35K0su3nFUnbBSJid0Q0IqIxMTEx5G4AAOYMO+jvk7TV9lLbqyWtkfTIkP8ZAIAepNbR294r\n6VpJy2xPS7pFs4uzfydpQtL9th+NiI0Rcdj2PklHJJ2RtC0izi7w1qgxHnICZMcRkXcf1Gg0otls\n5t0NZGT+9r7S7NYB7K0D9Mb2wYhopB3HpmbIHA85AbJF0CNzbO8LZIugR+bY3hfIFkHfhud/ZoOH\nnADZYvfKBM//XNwwq2TY3hfIFkGf4PmfCxvFhyDb+wLZYeomwQLhwqiSAcqNoE+wQLgwPgSBciPo\nEywQLowPQaDcCPrE5vWTum3L1ZocH5MlTY6Pcadmgg9BoNxYjG3DAmFnVMkA5UbQoyt8CALlxdQN\nAFQcQQ8AFUfQA0DFEfQAUHEsxgLoCk8FKy+CHkAqNv0rN6ZuAKRiv6NyI+gBpGK/o3Ij6AGkYr+j\nciPoAaRiv6NyYzEWQCr2Oyo3gh5AV9jvqLyYugGAiiPoAaDiCHoAqLjUoLe9x/Zx20+2tV1q+0Hb\n302+XtL22k7bx2wftb1xVB0HAHSnmzP6OyVdP69th6SvRcQaSV9LfpbtKyVtlXRV8jt32F4iAEBu\nUoM+Ih6SdGJe8yZJdyXf3yVpc1v7PRFxOiKeknRM0jVD6isAoA/9ztFfFhHfT75/XtJlyfeTkp5t\nO246abuA7RttN203Z2Zm+uwGACDNwIuxERGSoo/f2x0RjYhoTExMDNoNAMAC+g36F2xfLknJ1+NJ\ne0vSyrbjViRtAICc9Bv090l6X/L9+yR9qa19q+2ltldLWiPpkcG6CAAYROoWCLb3SrpW0jLb05Ju\nkTQlaZ/tGyQ9I+k9khQRh23vk3RE0hlJ2yLibMc3BgBkIjXoI+K9C7x03QLH3yrp1kE61S0ebQYA\n6Uq7qRmPNgOA7pR2CwQebQYA3Slt0PNoMwDoTmmDnkebAUB3Shv0PNoMALpT2sVYHm0GAN0pbdBL\nPNoMALpR2qkbAEB3CHoAqDiCHgAqjqAHgIoj6AGg4jz73JCcO2HPaHYXzCJYJukHeXei4BijdIxR\nOsYoXdoY/WJEpD65qRBBXyS2mxHRyLsfRcYYpWOM0jFG6YY1RkzdAEDFEfQAUHEE/YV2592BEmCM\n0jFG6RijdEMZI+boAaDiOKMHgIqrddDbHrf9edvfsf1t27/W9trHbIftZXn2MW8LjZHtDydth23/\ndd79zFOnMbK9zvbDth+13bR9Td79zIvttck4zP35se2P2L7U9oO2v5t8vSTvvuZpkXHalfzdetz2\nF22P9/zedZ66sX2XpP+IiM/YvljSKyPipO2Vkj4j6ZckvSUialvr22mMJK2XdLOk346I07ZfExHH\nc+1ojhYYo32Sbo+IL9t+h6SbIuLaPPtZBLaXSGpJ+hVJ2ySdiIgp2zskXRIRf5FrBwti3jitlXQg\nIs7Y/itJ6nWcantGb/vVkn5D0mclKSJ+FhEnk5dvl3STpPp+CmrRMfqQpKmIOJ201znkFxqjkPTz\nyWGvlvRcPj0snOsk/VdEPCNpk6S7kva7JG3OrVfF8/I4RcRXI+JM0v6wpBW9vlltg17Sakkzkv7R\n9iHbn7H9KtubJLUi4rGc+1cEHcdI0hsk/brtb9r+d9u/nG83c7XQGH1E0i7bz0r6lKSdeXayQLZK\n2pt8f1lEfD/5/nlJl+XTpUJqH6d2H5D05V7frM5B/wpJb5b06YhYL+n/JH1S0sclfSLHfhVJpzHa\nkbRfKulXJW2XtM+2c+tlvhYaow9J+mhErJT0USVn/HWWTGu9S9Ln5r8Ws3PItb6CnrPQONm+WdIZ\nSXf3+p51DvppSdMR8c3k589r9n/Y1ZIes/20Zi+RvmX7tfl0MXcLjdG0pHtj1iOSXtLsnhx1tNAY\nvU/SvUnb5yTVdjG2zW9J+lZEvJD8/ILtyyUp+VrbKcB55o+TbL9f0jsl/X70sbBa26CPiOclPWt7\n7mni12l2cF8TEasiYpVm/yd+c3Js7SwwRkck7Zf0Vkmy/QZJF6umm1MtMkbPSfrNpO1tkr6bQ/eK\n5r06fzriPs1+ICr5+qXMe1RM542T7es1u2b4roj4ST9vWPeqm3Wara65WNJ/S/rDiPhh2+tPS2rU\nvOrmgjHS7PTEHknrJP1M0p9HxIHcOpmzBcboKkl/q9mpnZ9K+uOIOJhbJ3OWrFt8T9LrIuJHSdsv\naLY66QrN7l77nog4kV8v87fAOB2TtFTS/ySHPRwRH+zpfesc9ABQB7WdugGAuiDoAaDiCHoAqDiC\nHgAqjqAHgIoj6AGg4gh6AKg4gh4AKu7/AVu043GM9v2fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9311710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.json\")\n",
    "\n",
    "_myDf= spark.read.json(jfile)\n",
    "_myDf.filter(_myDf['age'] > 21).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()  #dataframe으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)\n",
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> <type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(wc), type(wc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py:316: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF=spark.createDataFrame(wc)\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1905-5-5',\n",
       "  u'FullName': u'\\xc3ngel Bossio',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)\n",
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\session.py:336: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "wcSchema=StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "wcDF=spark.createDataFrame(wcRdd)\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print datetime.strptime(\"11/25/1991\", '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930, DoB=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF=wcDF.withColumn('DoB', wcDF['DateOfBirth'].cast(DateType()))\n",
    "wcDF=wcDF.withColumn('NumberInt', wcDF['Number'].cast(\"integer\"))\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=u'kim, js'),\n",
       " Row(name=u'lee, sm'),\n",
       " Row(name=u'lim, yg'),\n",
       " Row(name=u'lee')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF=wcDF.withColumnRenamed('ClubCountry','ClubNation')\n",
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| _1|     _2| _3|\n",
      "+---+-------+---+\n",
      "|  1|kim, js|170|\n",
      "|  1|lee, sm|175|\n",
      "|  2|lim, yg|180|\n",
      "|  2|    lee|170|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]\n",
    "myDf=spark.createDataFrame(myList)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n",
      "+----+-----------+------------+----------+\n",
      "|year|max(height)|max(heightD)|max(yearI)|\n",
      "+----+-----------+------------+----------+\n",
      "|   1|        175|       175.0|         1|\n",
      "|   2|        180|       180.0|         2|\n",
      "+----+-----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()\n",
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'createReplaceTempView'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-56db5ecc2841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwcDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#data tabl의 이름을 만듦\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select Club,Team,Year from wc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#가상 테이블 명과 동일하게\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 844\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'createReplaceTempView'"
     ]
    }
   ],
   "source": [
    "wcDF.createReplaceTempView(\"wc\")#data tabl의 이름을 만듦\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1) #가상 테이블 명과 동일하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\")#있으면 기존것 씀\n",
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio\n",
      "Full name: Juan Botasso\n",
      "Full name: Roberto Cherro\n",
      "Full name: Alberto Chividini\n",
      "Full name: \n"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0])\n",
    "for e in namesRdd.take(5):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "toDoublefunc = udf(lambda x: float(x),DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\",toDoublefunc(myDf.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+-------+-----+\n",
      "|year|   name|height|heightD|yearI|\n",
      "+----+-------+------+-------+-----+\n",
      "|   1|kim, js|   170|  170.0|    1|\n",
      "|   1|lee, sm|   175|  175.0|    1|\n",
      "|   2|lim, yg|   180|  180.0|    2|\n",
      "|   2|    lee|   170|  170.0|    2|\n",
      "+----+-------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType\n",
    "toint=udf(lambda x:int(x),IntegerType())\n",
    "myDf=myDf.withColumn(\"yearI\",toint(myDf['year']))\n",
    "\n",
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\201511142\\data\\kddcup.data_10_percent.gz data does not exist! retrieving..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')\n",
    "if(not os.path.exists(_fname)):\n",
    "    print \"%s data does not exist! retrieving..\" % _fname\n",
    "    _f=urllib.urlretrieve(_url,_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_rdd=spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n"
     ]
    }
   ],
   "source": [
    "_normal=_rdd.filter(lambda x:'normal.' in x)\n",
    "print _normal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "_csvRdd=_rdd.map(lambda x: x.split(','))\n",
    "print _csvRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_kv=_csvRdd.map(lambda x:(x[41],1))\n",
    "_attack=_kv.reduceByKey(lambda x,y:x+y) #groupByKey와 동일-전체를 셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attack.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n",
      "396743\n"
     ]
    }
   ],
   "source": [
    "_normalRdd=_csvRdd.filter(lambda x:x[41]==\"normal.\")\n",
    "_attackRdd=_csvRdd.filter(lambda x:x[41]!=\"normal.\")\n",
    "print _normalRdd.count()\n",
    "print _attackRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_csvRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
