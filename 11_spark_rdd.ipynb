{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=\"D:\\\\Code\\\\201511142\\\\spark-2.0.0-bin-hadoop2.6\"\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\n",
      "D:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\n",
      "C:\\Users\\400T6B\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\n",
      "C:\\Users\\400T6B\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\n",
      "\n",
      "C:\\Users\\400T6B\\Anaconda2\\python27.zip\n",
      "C:\\Users\\400T6B\\Anaconda2\\DLLs\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\plat-win\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\lib-tk\n",
      "C:\\Users\\400T6B\\Anaconda2\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\Sphinx-1.5.1-py2.7.egg\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\win32\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\win32\\lib\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\Pythonwin\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\setuptools-27.2.0-py2.7.egg\n",
      "C:\\Users\\400T6B\\Anaconda2\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\400T6B\\.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).config('spark.sql.warehouse.dir','file:///D:/Code/201511142/data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "117.16.43.165\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.get.\n: java.util.NoSuchElementException: spark.jars.packages\r\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:735)\r\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:735)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:735)\r\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:68)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9eea628f668f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.master'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.driver.host'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mprint\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.jars.packages'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\conf.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"default\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Code\\201511142\\spark-2.0.0-bin-hadoop2.6\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35.get.\n: java.util.NoSuchElementException: spark.jars.packages\r\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:735)\r\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$getConfString$2.apply(SQLConf.scala:735)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:735)\r\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:68)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')\n",
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)\n",
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475\n"
     ]
    }
   ],
   "source": [
    "print myRdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#myDf=spark.read.text(\".\\\\data\\\\ds_spark_wiki.txt\")\n",
    "myDf=spark.read.text(\"./data/ds_spark_wiki.txt\")\n",
    "print type(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n"
     ]
    }
   ],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "print myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2', u'40, 27', u'12, 38', u'15, 31', u'21, 1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd3 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "myRdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4=myRdd3.map(lambda line: line.split(','))\n",
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i+32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "f=map(c2f,celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x:(float(9)/5)*x+32,celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n"
     ]
    }
   ],
   "source": [
    "sentence='Hello World'\n",
    "words=sentence.split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x:x.split(),sentence)\n",
    "sentence=[\"Hello World\"]\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result=filter(lambda x:x%2,fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x,y:x+y,range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "nRdd=spark.sparkContext.parallelize([1,2,3,4])\n",
    "squared=nRdd.map(lambda x:x*x).collect()\n",
    "print squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 4 1.11803398875 1.25\n"
     ]
    }
   ],
   "source": [
    "print nRdd.sum(),nRdd.min(),nRdd.max(),nRdd.stdev(),nRdd.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=myRdd2.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-689e2fe9025f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyRdd2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ds_spark_wiki.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mmyRdd2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "print myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "words=myRdd2.map(mySplit)\n",
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in words.collect():\n",
    "    for word in line:\n",
    "        print word,\n",
    "    print \"\\n-----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print myRdd_unicode.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['is','am','are','the','for','a']\n",
    "myRdd_stop = myRdd2.filter(lambda x: x not in stopwords)\n",
    "myRdd_stop.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'AMPLab,', 1)\n",
      "(u'Apache', 6)\n",
      "(u\"Berkeley's\", 1)\n",
      "(u'California,', 1)\n",
      "(u'Foundation,', 1)\n",
      "(u'Originally', 1)\n",
      "(u'Software', 1)\n",
      "(u'Spark', 7)\n",
      "(u'University', 1)\n",
      "(u'Wikipedia', 1)\n"
     ]
    }
   ],
   "source": [
    "for e in wc:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc2=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "(7, u'Spark')\n",
      "(6, u'Apache')\n",
      "(5, u'\\uc544\\ud30c\\uce58')\n",
      "(4, u'\\uc2a4\\ud30c\\ud06c')\n",
      "(3, u'the')\n",
      "(2, u'an')\n",
      "(1, u'and')\n",
      "(1, u'\\uc18c\\uc2a4')\n",
      "(1, u'is')\n",
      "(1, u'Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "print type(wc2)\n",
    "for i in wc2:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAD8CAYAAABKKbKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x62c8550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = map(lambda x: x[0], wc2)\n",
    "word = map(lambda x: x[1], wc2)\n",
    "plt.barh(range(len(count)), count, color = 'grey')\n",
    "plt.yticks(range(len(count)), word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds_spark_rdd_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_hello.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!D:/Code/201511142/spark-2.0.0-bin-hadoop2.6/bin/spark-submit src/ds_spark_rdd_hello.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
